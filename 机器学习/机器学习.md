# 机器学习

## 1、监督学习（supervised learning）

**监督学习的定义**：向机器给定正确的输出和输入的映射关系的示例进行学习，使得机器可以在只给定输入的情况下，给出合理且准确的输出预测和猜测。（由已知推未知）

### 1.1 回归（regression）

例如：房价和房屋面积的关系图

![img](https://i-blog.csdnimg.cn/direct/14de3b1418d5498aad37b25f9ef0ade7.png)

在该例子中，通过对离散的房价数据进行线性和非线性拟合**（回归regression）**，从而可以通过一个输入（房屋面积）从而得到相应的无限多个可能的输出（房价）。

### 1.2 分类（classification）

例如：通过肿瘤尺寸判断肿瘤是良性还是恶性的关系图

![img](https://i-blog.csdnimg.cn/direct/e1080d176d0648bfa72aee8bd449a315.png)

 在单一特征的情况下，输入可以是任意的，但是输出只能为良性或者恶性，类似数电中的高低电平。

![img](https://i-blog.csdnimg.cn/direct/94c12f85ba274380a529ba3f8a41f7bd.png)

 在有多输入的情况下，学习算法将给出一个边界线（可以理解为两点确定一条直线）用以区分肿瘤的良性和恶性。

综上，通过更多的输入参数，我们可以得到更加准确合理的输出结果，而更多的输出类型，可以帮助我们更加全面的定义**监督学习**的结果。不管是回归还是**分类**，监督学习都需要事先给定正确的**输出和输入**的映射关系的示例进行学习，进而可以自己独立的给出相对正确的结果或推测。

## 2、无监督学习

处理的是**无标签 (unlabeled)** 的数据。如右图所示，我们只有一堆数据点，没有任何关于它们类别的信息。算法的目标是在这些数据中自主地发现有趣的结构或模式。最常见的任务之一就是将数据分成不同的簇（cluster），这个过程也叫做**聚类 (Clustering)**



![image-20260117181125647](https://img.gostarlight.top:52988/i/2026/01/17/ttje7w-2.png)

### 2.1 聚类

聚类是无监督学习中最核心、最广泛应用的技术之一。它的目标是将相似的数据点归到同一个组（或簇）里。

> 例如：
>
> **谷歌新闻 (Google News)**
> 每天，互联网上都会涌现出成千上万篇关于同一事件的新闻报道。谷歌新闻的一个核心功能就是自动将这些报道聚合在一起，形成一个新闻专题。这就是一个典型的聚类应用：算法自动分析文章内容，将讲述同一个故事的文章聚集到一起。
>
> **基因芯片 (DNA Microarray)**
> 在生物信息学领域，研究人员可以使用基因芯片技术测量成千上万个基因在不同个体中的表达水平。通过聚类算法，我们可以根据基因表达的相似性将不同的个体进行分组，这可能帮助我们识别出不同的疾病亚型或发现具有特定生物学意义的基因模式。

## 3、线性回归模型

### 3.1 什么是回归问题

我们的目标是建立一个模型，它能根据房屋的面积（size）来预测其价格（price）。

**回归模型 (Regression model)**：预测的是一个连续的数值（Numbers），理论上有无限多种可能的输出。
**分类模型 (Classification model)**：预测的是一个离-散的类别（Categories），可能的输出数量是有限的。
而这两种模型都属于监督学习模型 (Supervised learning model)，因为我们用来训练它们的数据都包含了“正确答案”。

### 3.2 模型的专业术语

我们用来构建模型的数据，通常被称为**数据集 (Data set)**。在我们的例子中，数据集就是一系列房屋面积及其对应的真实价格。

![image-20260125211858372](机器学习.assets\image-20260125211858372.png)

这张图非常直观地展示了数据点和数据表之间的关系。图上的每一个红叉，都对应着数据表中的一行。

为了更精确地描述，吴恩达老师引入了一套标准的数学符号：

> x: 表示输入变量 (input variable)，也常被称为特征 (feature)。在我们的例子中，x就是房屋的面积。
> y: 表示输出变量 (output variable)，也常被称为目标 (target)。在我们的例子中，y就是房屋的价格。
> m: 表示训练样本的数量 (number of training examples)。图中的例子里，我们有47个训练样本，所以 m=47。
> (x, y): 表示一个单个的训练样本 (single training example)。
> (x⁽ⁱ⁾, y⁽ⁱ⁾): 表示第 i 个训练样本。这里的上标 (i) 只是一个索引 (index)，代表第几行数据，它不是数学上的指数或次方。
> 例如，(x⁽¹⁾, y⁽¹⁾) 就代表第一个训练样本，即 (2104, 400)。

### 3.3 模型的表示 (Model Representation)

![image-20260125211843149](机器学习.assets\image-20260125211843149.png)

上图左侧清晰地展示了模型训练和预测的流程：

1. 我们将训练集 (training set)（包含了特征 x 和目标 y）喂给一个学习算法 (learning algorithm)。
2. 学习算法会输出一个函数 f。这个函数 f 就是我们最终得到的模型 (model)，在一些文献中它也被称为假设 (hypothesis)。
3. 当有新的输入 x（例如，一个全新的房屋面积）时，我们就可以用这个模型 f 来进行预测。模型输出的预测值，我们通常用 ŷ (读作 “y-hat”) 来表示。

那么，这个模型 f 的具体形式是什么呢？对于线性回归，我们希望找到一条直线来拟合我们的数据。初中数学告诉我们，一条直线的方程可以表示为 y = wx + b。在机器学习中，我们沿用这个形式，将模型写作：

`f_w,b(x) = wx + b`

这个模型只有一个输入特征 x（房屋面积），因此它被称为单变量线性回归 (Univariate linear regression)。“Univariate”中的“uni”就是“单一”的意思。我们的核心任务，就是通过学习算法，找到最优的参数 w 和 b，使得这条直线能够最好地拟合我们的训练数据。
## 4、代价函数

在上文我们定义了线性回归模型 f(x) = wx + b。我们知道，通过调整参数 w 和 b，我们可以得到不同的直线。那么，接下来的问题是：如何自动地为我们的模型找到最合适的 w 和 b 呢？

要回答这个问题，我们首先需要一种方法来衡量模型的好坏。这就是**代价函数（Cost Function）**的作用。

### 4.1 线性函数

假设我们有一个包含输入特征 \( x \) 和输出目标 \( y \) 的训练集。用于拟合这个训练集的模型是一个线性函数：
$$
f_{w,b}(x) = wx + b
$$
其中，\( w \) 和 \( b \) 被称为模型的参数。在机器学习中，模型的参数是在训练期间可以调整的变量，以改进模型的性能。有时，\( w \) 和 \( b \) 也被称为**系数**或**权重**。

通过下面这组图，我们可以非常直观地理解它们的作用：

![image-20260125231513836](机器学习.assets\image-20260125231513836.png)

- **b (y-intercept)**：当 `w=0` 时，`f(x) = b`，这是一条水平线。`b` 决定了直线与y轴的截距。
- **w (slope)**：`w` 决定了直线的斜率。它表示 `x` 每增加1，`f(x)` 变化的量。`w` 越大，直线越陡峭。

通过选择不同的 `w` 和 `b`，我们的直线模型就能以不同的方式去拟合数据。我们的目标，就是找到那个能“最好地”拟合数据的 `w` 和 `b`。

### 4.2 定义代价函数 J(w, b)

为了衡量拟合的好坏，我们需要量化模型预测值与真实值之间的“差距”或“误差”。

![image-20260125231752665](机器学习.assets\image-20260125231752665.png)

对于第 i 个训练样本$ (x⁽ⁱ⁾, y⁽ⁱ⁾)$：

真实值是$y⁽ⁱ⁾$。
模型的预测值是 $ŷ⁽ⁱ⁾ = f(x⁽ⁱ⁾) = wx⁽ⁱ⁾ + b$。
两者之间的误差就是 $ŷ⁽ⁱ⁾ - y⁽ⁱ⁾$。
为了得到一个总体的误差评估，一个非常有效且常用的方法是计算所有样本误差的平方和的平均值。这就是我们的代价函数，通常用 J(w, b) 来表示：
$$
J(w, b) = \frac{1}{2m} \sum_{i=1}^{m} \left(\hat{y}^{(i)} - y^{(i)}\right)^2
$$

> - 额外除以 2 只是为了让我们后面的一些计算看起来更整洁，但无论是否包含此除以 2，代价函数仍然有效。
> - 对误差取平方。这有两个好处：
>   - 可以确保误差值为正，避免正负误差相互抵消。
>   - 可以放大较大误差的惩罚，使得模型更倾向于去拟合那些偏差大的点。

我们的目标是通过调整来 `w` 和 `b `最小化代价函数，从而使模型的预测更加准确。

### 4.3 直观理解代价函数

代价函数用于衡量模型预测值与实际值之间的差异。具体来说，线性回归的目标是找到参数 $w$ 和 $b$，使得代价函数 $J(w,b)$ 最小化。数学上，我们表示为：

$$
\mathop{\text{minimize}}\limits_{w,b} J(w,b)
$$
![image-20260125234616476](机器学习.assets\image-20260125234616476.png)

为了更直观地理解代价函数，我们暂时简化模型，仅考虑参数 $w$，即假设 $b=0$。此时，模型变为：

$$
f_{w}(x)=w \cdot x
$$

相应的代价函数也简化为：

$$
J(w)=\frac{1}{2m}\sum^{m}_{i=1}(w \cdot x^{(i)}-y^{(i)})^2
$$

假设我们有一个简单的训练集，包含三个数据点：$(1, 1)$、$(2, 2)$ 和 $(3, 3)$。

不同 $w$ 值下的代价

**$w = 1$ 时**

模型 $f_{w}(x)$ 是一条斜率为 1 的直线，完美地通过所有数据点。  
计算代价函数 $J(w)$：$J(1) = 0$，表示模型完美拟合数据。

![image-20260125234904696](机器学习.assets\image-20260125234904696.png)

**$w = 0.5$ 时**

模型 $f_{w}(x)$ 是一条斜率为 0.5 的直线，未能完全拟合数据。  
计算代价函数 $J(w)$：$J(0.5) \approx 0.58$，表示模型拟合效果较差。

![image-20260125234912618](机器学习.assets\image-20260125234912618.png)

**$w = 0$ 时**

模型 $f_{w}(x)$ 是一条水平线，完全偏离数据。  
计算代价函数 $J(w)$：$J(0) \approx 2.33$，表示模型拟合效果非常差。

![image-20260125234919431](机器学习.assets\image-20260125234919431.png)

**结论**：  
这就是在线性回归中，如何使用代价函数来找到使 $J$ 最小化的 $w$ 值。在更一般的情况下，我们有参数 $w$ 和 $b$ 而不仅仅是 $w$，需要找到使 $J$ 最小化的 $w$ 和 $b$ 的值。

### 4.4 可视化代价函数

现在，我们将回到完整的线性回归模型，同时考虑参数 $w$和$ b$，并可视化代价函数来深入理解其作用。

假设我们有一个房价预测模型，输入特征 $x$ 表示房屋的大小（单位：平方米），输出目标 $y$ 表示房屋的价格（单位：万元）。

我们选择参数 $w = 0.06$ 和 $b = 50$，则模型函数为：

$$
f_{w,b}(x) = 0.06 \cdot x + 50
$$

这个模型对房价的预测效果较差，因为它始终低估了房价（对于大多数样本，预测值 $f_{w,b}(x)$ 小于真实价格 $y$）。

![image-20260126134653834](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20260126134653834.png)

 在此之前，我们将参数$b$设为 0 来简化模型，这使得代价函数$J ( w )$成为一个二维的 U 形曲线，形状类似于一个“汤碗”。然而，在完整的线性回归模型中，我们需要同时考虑参数 $w$ 和 $b$，这使得代价函数 $J ( w , b )$ 成为一个三维的曲面。

![image-20260126134815080](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20260126134815080.png)

#### 4.4.1 可视化工具：等高线图 (Contour Plot)

3D图像虽然直观，但在纸面上绘制和分析并不方便。因此，我们常常使用**等高线图**来从“正上方”俯视这个3D曲面。

<img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20260126134943122.png" alt="image-20260126134943122"  />

等高线图就像我们地理课上学的地形图：

- 图上的每一个椭圆（等高线），都代表着一组能得到相同代价值 J 的 (w, b) 组合。
- 最中心的那个点，就是“碗”的最低点，也就是我们寻找的代价函数的最小值点。

通过下面这组动图，我们可以清晰地看到模型、代价函数曲面和等高线图之间的联动关系：

**一个糟糕的拟合**：

![image-20260126135145597](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20260126135145597.png)

当 `w=-0.15, b=500` 时，我们在等高线图上处于一个远离中心的位置。对应的，左边的拟合直线与数据点偏差很大，误差很高，因此在3D曲面上的位置也很高。

**一个更好的拟合**：

![image-20260126135227934](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20260126135227934.png)

当我们选择 `w=0.13, b=71`，这个点在等高线图上更靠近中心。此时，左边的拟合直线与数据点的贴合程度明显变好，总误差（代价 `J`）也显著降低。

我们的最终目标，就是找到等高线图最中心点对应的 `(w, b)`，因为它能让我们的模型产生的总误差最小，也就是对数据拟合得最好。

下一篇文章，我们将介绍一种强大的算法——**梯度下降**，它能够自动地、高效地为我们找到这个代价函数的最低点。

## 5、梯度下降

在上一篇文章中，我们定义了代价函数$ J(w, b)$，并明确了我们的目标是找到能使其最小化的参数 $w$ 和 $b$。但是，我们如何才能系统性地、自动地找到这个最小值点呢？

答案就是**梯度下降（Gradient Descent算法）**。这是一个应用极其广泛的优化算法，不仅用于线性回归，也贯穿于后续更复杂的机器学习模型中。

### 5.1 核心思想

![image-20260126153927702](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20260126153927702.png)

概括来说，算法的执行流程如下：

1. 初始化：随机选择一个起始点，即为 w 和 b 赋一个初始值（通常设为0）。
2. 迭代更新：持续地、小步地改变 w 和 b，确保每一步都朝着使代价函数 J(w, b) 减小的方向前进。
3. 收敛：当 J(w, b) 的值不再显著下降，或者说我们到达了一个（局部）最低点时，算法停止。
   

> 为了更形象地理解这个过程，吴恩达老师给出了一个非常经典的比喻：
>
> ![image-20260126154040830](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20260126154040830.png)
>
> 想象一下你正站在一座崎岖不平的山上（这座山就是我们的代价函数曲面），你的目标是尽快走到山谷的最低点。由于雾很大，你无法直接看到最低点在哪里。
>
> 一个合理的策略是：环顾四周，找到当前位置最陡峭的下山方向，然后朝着这个方向迈出一步。到达新位置后，再次重复这个过程：环顾四周，找最陡的下山方向，再迈一步。
>
> 持续这个过程，你最终就会走到一个山谷的谷底，即一个局部最低点（local minima）。这就是梯度下降算法的核心直觉。

### 5.2 算法详解

梯度下降的核心思想是通过**迭代更新**参数 $w$ 和 $b$，使得代价函数 $J(w,b)$ 逐渐减小，最终收敛到（局部）最小值。

梯度下降的更新规则如下：

**1. 更新参数 $w$**
$$
w := w - \alpha \cdot \frac{\partial J(w,b)}{\partial w}
$$

**2. 更新参数 $b$**
$$
b := b - \alpha \cdot \frac{\partial J(w,b)}{\partial b}
$$

梯度下降的核心思想是通过迭代更新参数 *w* 和 *b*，使得代价函数 *J*(*w*,*b*) 逐渐减小。具体来说，梯度下降的更新规则如下：

**1. 赋值运算符 `:=`**

- 在算法和编程中，`:=` 表示**赋值操作**（如 Python 中的 `=`）。
- 例如，$w := w - \alpha \cdot \frac{\partial J(w,b)}{\partial w}$ 表示将 $w$ 的当前值更新为右侧表达式的计算结果。
- **注意**：这与数学中的等号 `=` 不同，后者通常用于表示**真值断言**（如 $2+2=4$）。

**2. 学习率 $\alpha$**

- 学习率 $\alpha$ 是一个介于 0 和 1 之间的正数，通常设置为 0.01 或通过实验确定。
- **作用**：决定了每次参数更新的步长。
  - **较大的 $\alpha$**：更激进的更新，可能收敛更快，但容易“震荡”甚至发散。
  - **较小的 $\alpha$**：更谨慎的更新，收敛稳定但可能较慢。

**3. 偏导数 $\frac{\partial J}{\partial w}$ 和 $\frac{\partial J}{\partial b}$**

- **含义**：表示代价函数在 $w$ 和 $b$ 方向上的**变化率**（梯度）。
- **作用**：
  1. **方向**：告诉我们参数应该朝哪个方向更新（正梯度→减小 $w/b$，负梯度→增大 $w/b$）。
  2. **大小**：梯度越大，说明当前参数位置离最优解越远，需要更大的调整。

![image-20260126161840374](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20260126161840374.png)

在实现梯度下降时，一个关键细节是**同步更新**参数 *w* 和 *b*。这意味着在每次迭代中，我们需要同时计算 *w* 和 *b* 的更新值，然后再同时更新它们。上图左边展示了正确的更新步骤，右边则是不推荐的错误做法。

### 5.3 理解梯度下降

为了更深入地理解这个更新公式为什么能奏效，我们再次简化问题，暂时只考虑参数 `w`，即最小化 `J(w)`。此时，代价函数 $J(w)$ 是一个关于 $w$ 的一维函数，其图形是一条曲线。梯度下降的更新规则简化为：

$$
w := w - \alpha \cdot \frac{d J(w)}{d w}
$$

其中，$\alpha$ 是学习率，$\frac{d J(w)}{d w}$ 是代价函数 $J(w)$ 对 $w$ 的导数，表示代价函数在 $w$ 方向上的变化率。具体来说，导数告诉我们 $w$ 应该如何更新，以最快地降低代价函数。

![image-20260126164733305](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20260126164733305.png)

**导数为正时**

- 如果导数为正，意味着代价函数在当前 $w$ 处是上升的。  
- 根据梯度下降的更新规则，$w$ 会减小（即 $w := w - \alpha \cdot \text{正数}$）。  
- 在图形上，$w$ 会向左移动，代价函数 $J(w)$ 会逐渐减小。

**导数为负时**

- 如果导数为负，意味着代价函数在当前 $w$ 处是下降的。  
- 根据梯度下降的更新规则，$w$ 会增加（即 $w := w - \alpha \cdot \text{负数}$）。  
- 在图形上，$w$ 会向右移动，代价函数 $J(w)$ 会逐渐减小。

通过这种方式，导数项引导 $w$ 朝着代价函数的最小值方向移动。

### 5.4 学习率

**（1）学习率过小**

如果学习率 $\alpha$ 过小，梯度下降的更新步长会非常小。具体来说：

- **更新步长小**：每次更新 $w$ 时，$w$ 的变化量非常小。
- **收敛速度慢**：虽然梯度下降最终会收敛到最小值，但需要非常多的迭代步骤。
- **效率低下**：计算成本高，尤其是在大规模数据集上。

**示例**：假设学习率 $\alpha = 0.0000001$，每次更新 $w$ 的步长非常小。虽然 $w$ 会逐渐接近最小值，但需要大量的迭代步骤才能达到目标。

**（2）学习率过大**

如果学习率 $\alpha$ 过大，梯度下降的更新步长会非常大。具体来说：

- **更新步长大**：每次更新 $w$ 时，$w$ 的变化量非常大。
- **可能无法收敛**：梯度下降可能在最小值附近振荡，甚至偏离最小值。
- **发散风险**：在某些情况下，梯度下降可能完全无法收敛，导致代价函数值不断增加。

**示例**：假设学习率 $\alpha = 10$，每次更新 $w$ 的步长非常大。梯度下降可能会从最小值的一侧跳到另一侧，甚至偏离最小值，导致代价函数值不断增加。

![image-20260126181831483](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20260126181831483.png)

**（3）学习率的自动调整**

一个有趣的现象是，即使学习率 $\alpha$ 保持不变，梯度下降在接近最小值时也会自动减小更新步长。这是因为：

- **导数变小**：当 $w$ 接近最小值时，导数 $\frac{d J(w)}{d w}$ 会逐渐变小。
- **更新步长减小**：由于更新步长 $\alpha \cdot \frac{d J(w)}{d w}$ 中的导数项变小，更新步长也会自动减小。
- **稳定收敛**：这使得梯度下降在接近最小值时能够稳定地收敛，而不会在最小值附近振荡。

![image-20260126182022029](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20260126182022029.png)

**（4）局部最小值**

当 $w$ 处于局部最小值时，导数 $\frac{d J(w)}{d w}$ 为零。此时，梯度下降的更新规则变为：

$$
w := w - \alpha \cdot 0 = w
$$

这意味着，如果 $w$ 已经处于局部最小值，梯度下降不会改变 $w$ 的值，算法会保持稳定。

![image-20260126182115064](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20260126182115064.png)

### 5.5 梯度下降用于线性回归

现在，我们将这个通用的梯度下降算法，应用到我们之前为线性回归定义的平方误差代价函数上。

#### 5.5.1 线性回归的梯度

![image-20260126192730306](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20260126192730306.png)

线性回归中代价函数 $J(w, b)$ 的偏导数公式为：

$$
\frac{\partial}{\partial w} J(w,b) = \frac{1}{m} \sum_{i=1}^{m} \left( f_{w,b}(x^{(i)}) - y^{(i)} \right) \cdot x^{(i)}
$$

$$
\frac{\partial}{\partial b} J(w,b) = \frac{1}{m} \sum_{i=1}^{m} \left( f_{w,b}(x^{(i)}) - y^{(i)} \right)
$$

其中，$f_{w,b}(x^{(i)}) = w x^{(i)} + b$。将这两个具体的导数公式代入到通用的梯度下降更新规则中，我们就得到了线性回归的完整学习算法。

#### 5.5.2 凸函数与全局最优解

我们之前提到，梯度下降可能会陷入**局部最低点**。

![image-20260126192910147](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20260126192910147.png)

幸运的是，用于线性回归的平方误差代价函数是一个**凸函数（Convex function）**。从图形上看，它是一个完美的“碗”形，没有任何局部的凹陷。

![image-20260126192936780](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20260126192936780.png)

这意味着它**只有一个最低点，这个点既是局部最低点，也是全局最低点（global minimum）**。

因此，只要我们正确地使用梯度下降算法，就**一定能**为线性回归找到全局最优的参数 `w` 和 `b`。

#### 5.5.3 梯度下降运行过程

![image-20260126193842592](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20260126193842592.png)

上图生动地展示了梯度下降在线性回归代价函数上的工作过程。随着迭代的进行，参数 `(w, b)` 在右侧的等高线图上一步步走向中心最低点，同时，左侧的模型拟合直线也在数据点中逐步调整到最优的位置。

**“批”梯度下降 (Batch Gradient Descent)**：

这里的“批（Batch）”指的是，在计算梯度并更新参数的每一步中，我们都使用了全部（整个一批的训练样本（从 i=1 到 m）。

在后续的课程中，我们还会接触到其他类型的梯度下降算法，它们在每一步中可能只使用训练集的一个子集。但目前，我们所说的梯度下降，默认指的就是“批”梯度下降。

![image-20260126194059968](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.assets/image-20260126194059968.png)
