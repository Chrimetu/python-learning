# 机器学习

## 1、监督学习（supervised learning）

**监督学习的定义**：向机器给定正确的输出和输入的映射关系的示例进行学习，使得机器可以在只给定输入的情况下，给出合理且准确的输出预测和猜测。（由已知推未知）

### 1.1 回归（regression）

例如：房价和房屋面积的关系图

![img](https://i-blog.csdnimg.cn/direct/14de3b1418d5498aad37b25f9ef0ade7.png)

在该例子中，通过对离散的房价数据进行线性和非线性拟合**（回归regression）**，从而可以通过一个输入（房屋面积）从而得到相应的无限多个可能的输出（房价）。

### 1.2 分类（classification）

例如：通过肿瘤尺寸判断肿瘤是良性还是恶性的关系图

![img](https://i-blog.csdnimg.cn/direct/e1080d176d0648bfa72aee8bd449a315.png)

 在单一特征的情况下，输入可以是任意的，但是输出只能为良性或者恶性，类似数电中的高低电平。

![img](https://i-blog.csdnimg.cn/direct/94c12f85ba274380a529ba3f8a41f7bd.png)

 在有多输入的情况下，学习算法将给出一个边界线（可以理解为两点确定一条直线）用以区分肿瘤的良性和恶性。

综上，通过更多的输入参数，我们可以得到更加准确合理的输出结果，而更多的输出类型，可以帮助我们更加全面的定义**监督学习**的结果。不管是回归还是**分类**，监督学习都需要事先给定正确的**输出和输入**的映射关系的示例进行学习，进而可以自己独立的给出相对正确的结果或推测。

## 2、无监督学习

处理的是**无标签 (unlabeled)** 的数据。如右图所示，我们只有一堆数据点，没有任何关于它们类别的信息。算法的目标是在这些数据中自主地发现有趣的结构或模式。最常见的任务之一就是将数据分成不同的簇（cluster），这个过程也叫做**聚类 (Clustering)**



![image-20260117181125647](https://img.gostarlight.top:52988/i/2026/01/17/ttje7w-2.png)

### 2.1 聚类

聚类是无监督学习中最核心、最广泛应用的技术之一。它的目标是将相似的数据点归到同一个组（或簇）里。

> 例如：
>
> **谷歌新闻 (Google News)**
> 每天，互联网上都会涌现出成千上万篇关于同一事件的新闻报道。谷歌新闻的一个核心功能就是自动将这些报道聚合在一起，形成一个新闻专题。这就是一个典型的聚类应用：算法自动分析文章内容，将讲述同一个故事的文章聚集到一起。
>
> **基因芯片 (DNA Microarray)**
> 在生物信息学领域，研究人员可以使用基因芯片技术测量成千上万个基因在不同个体中的表达水平。通过聚类算法，我们可以根据基因表达的相似性将不同的个体进行分组，这可能帮助我们识别出不同的疾病亚型或发现具有特定生物学意义的基因模式。

## 3、线性回归模型

### 3.1 什么是回归问题

我们的目标是建立一个模型，它能根据房屋的面积（size）来预测其价格（price）。

**回归模型 (Regression model)**：预测的是一个连续的数值（Numbers），理论上有无限多种可能的输出。
**分类模型 (Classification model)**：预测的是一个离-散的类别（Categories），可能的输出数量是有限的。
而这两种模型都属于监督学习模型 (Supervised learning model)，因为我们用来训练它们的数据都包含了“正确答案”。

### 3.2 模型的专业术语

我们用来构建模型的数据，通常被称为**数据集 (Data set)**。在我们的例子中，数据集就是一系列房屋面积及其对应的真实价格。

![image-20260125211858372](D:\PersonData\star-blog\机器学习\机器学习.assets\image-20260125211858372.png)

这张图非常直观地展示了数据点和数据表之间的关系。图上的每一个红叉，都对应着数据表中的一行。

为了更精确地描述，吴恩达老师引入了一套标准的数学符号：

> x: 表示输入变量 (input variable)，也常被称为特征 (feature)。在我们的例子中，x就是房屋的面积。
> y: 表示输出变量 (output variable)，也常被称为目标 (target)。在我们的例子中，y就是房屋的价格。
> m: 表示训练样本的数量 (number of training examples)。图中的例子里，我们有47个训练样本，所以 m=47。
> (x, y): 表示一个单个的训练样本 (single training example)。
> (x⁽ⁱ⁾, y⁽ⁱ⁾): 表示第 i 个训练样本。这里的上标 (i) 只是一个索引 (index)，代表第几行数据，它不是数学上的指数或次方。
> 例如，(x⁽¹⁾, y⁽¹⁾) 就代表第一个训练样本，即 (2104, 400)。

### 3.3 模型的表示 (Model Representation)

![image-20260125211843149](D:\PersonData\star-blog\机器学习\机器学习.assets\image-20260125211843149.png)

上图左侧清晰地展示了模型训练和预测的流程：

1. 我们将训练集 (training set)（包含了特征 x 和目标 y）喂给一个学习算法 (learning algorithm)。
2. 学习算法会输出一个函数 f。这个函数 f 就是我们最终得到的模型 (model)，在一些文献中它也被称为假设 (hypothesis)。
3. 当有新的输入 x（例如，一个全新的房屋面积）时，我们就可以用这个模型 f 来进行预测。模型输出的预测值，我们通常用 ŷ (读作 “y-hat”) 来表示。

那么，这个模型 f 的具体形式是什么呢？对于线性回归，我们希望找到一条直线来拟合我们的数据。初中数学告诉我们，一条直线的方程可以表示为 y = wx + b。在机器学习中，我们沿用这个形式，将模型写作：

`f_w,b(x) = wx + b`

这个模型只有一个输入特征 x（房屋面积），因此它被称为单变量线性回归 (Univariate linear regression)。“Univariate”中的“uni”就是“单一”的意思。我们的核心任务，就是通过学习算法，找到最优的参数 w 和 b，使得这条直线能够最好地拟合我们的训练数据。
## 4、代价函数

在上文我们定义了线性回归模型 f(x) = wx + b。我们知道，通过调整参数 w 和 b，我们可以得到不同的直线。那么，接下来的问题是：如何自动地为我们的模型找到最合适的 w 和 b 呢？

要回答这个问题，我们首先需要一种方法来衡量模型的好坏。这就是**代价函数（Cost Function）**的作用。

### 4.1 线性函数

假设我们有一个包含输入特征 \( x \) 和输出目标 \( y \) 的训练集。用于拟合这个训练集的模型是一个线性函数：
$$
f_{w,b}(x) = wx + b
$$
其中，\( w \) 和 \( b \) 被称为模型的参数。在机器学习中，模型的参数是在训练期间可以调整的变量，以改进模型的性能。有时，\( w \) 和 \( b \) 也被称为**系数**或**权重**。

通过下面这组图，我们可以非常直观地理解它们的作用：

![image-20260125231513836](D:\PersonData\star-blog\机器学习\机器学习.assets\image-20260125231513836.png)

- **b (y-intercept)**：当 `w=0` 时，`f(x) = b`，这是一条水平线。`b` 决定了直线与y轴的截距。
- **w (slope)**：`w` 决定了直线的斜率。它表示 `x` 每增加1，`f(x)` 变化的量。`w` 越大，直线越陡峭。

通过选择不同的 `w` 和 `b`，我们的直线模型就能以不同的方式去拟合数据。我们的目标，就是找到那个能“最好地”拟合数据的 `w` 和 `b`。

### 4.2 定义代价函数 J(w, b)

为了衡量拟合的好坏，我们需要量化模型预测值与真实值之间的“差距”或“误差”。

![image-20260125231752665](D:\PersonData\star-blog\机器学习\机器学习.assets\image-20260125231752665.png)

对于第 i 个训练样本$ (x⁽ⁱ⁾, y⁽ⁱ⁾)$：

真实值是$y⁽ⁱ⁾$。
模型的预测值是 $ŷ⁽ⁱ⁾ = f(x⁽ⁱ⁾) = wx⁽ⁱ⁾ + b$。
两者之间的误差就是 $ŷ⁽ⁱ⁾ - y⁽ⁱ⁾$。
为了得到一个总体的误差评估，一个非常有效且常用的方法是计算所有样本误差的平方和的平均值。这就是我们的代价函数，通常用 J(w, b) 来表示：
$$
J(w, b) = \frac{1}{2m} \sum_{i=1}^{m} \left(\hat{y}^{(i)} - y^{(i)}\right)^2
$$

> - 额外除以 2 只是为了让我们后面的一些计算看起来更整洁，但无论是否包含此除以 2，代价函数仍然有效。
> - 对误差取平方。这有两个好处：
>   - 可以确保误差值为正，避免正负误差相互抵消。
>   - 可以放大较大误差的惩罚，使得模型更倾向于去拟合那些偏差大的点。

我们的目标是通过调整来 `w` 和 `b `最小化代价函数，从而使模型的预测更加准确。

### 4.3 直观理解代价函数

代价函数用于衡量模型预测值与实际值之间的差异。具体来说，线性回归的目标是找到参数 $w$ 和 $b$，使得代价函数 $J(w,b)$ 最小化。数学上，我们表示为：

$$
\mathop{\text{minimize}}\limits_{w,b} J(w,b)
$$
![image-20260125234616476](D:\PersonData\star-blog\机器学习\机器学习.assets\image-20260125234616476.png)

为了更直观地理解代价函数，我们暂时简化模型，仅考虑参数 $w$，即假设 $b=0$。此时，模型变为：

$$
f_{w}(x)=w \cdot x
$$

相应的代价函数也简化为：

$$
J(w)=\frac{1}{2m}\sum^{m}_{i=1}(w \cdot x^{(i)}-y^{(i)})^2
$$

假设我们有一个简单的训练集，包含三个数据点：$(1, 1)$、$(2, 2)$ 和 $(3, 3)$。

不同 $w$ 值下的代价

**$w = 1$ 时**

模型 $f_{w}(x)$ 是一条斜率为 1 的直线，完美地通过所有数据点。  
计算代价函数 $J(w)$：$J(1) = 0$，表示模型完美拟合数据。

![image-20260125234904696](D:\PersonData\star-blog\机器学习\机器学习.assets\image-20260125234904696.png)

**$w = 0.5$ 时**

模型 $f_{w}(x)$ 是一条斜率为 0.5 的直线，未能完全拟合数据。  
计算代价函数 $J(w)$：$J(0.5) \approx 0.58$，表示模型拟合效果较差。

![image-20260125234912618](D:\PersonData\star-blog\机器学习\机器学习.assets\image-20260125234912618.png)

**$w = 0$ 时**

模型 $f_{w}(x)$ 是一条水平线，完全偏离数据。  
计算代价函数 $J(w)$：$J(0) \approx 2.33$，表示模型拟合效果非常差。

![image-20260125234919431](D:\PersonData\star-blog\机器学习\机器学习.assets\image-20260125234919431.png)

**结论**：  
这就是在线性回归中，如何使用代价函数来找到使 $J$ 最小化的 $w$ 值。在更一般的情况下，我们有参数 $w$ 和 $b$ 而不仅仅是 $w$，需要找到使 $J$ 最小化的 $w$ 和 $b$ 的值。
